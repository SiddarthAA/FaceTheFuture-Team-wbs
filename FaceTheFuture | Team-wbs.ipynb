{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dde929a129bb483191fdd0b39703f49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd40f35353ef4f4ab7726e4c74807487",
              "IPY_MODEL_2b0d1c297e1640a7a3a80b1c6b76e534",
              "IPY_MODEL_a90752c370f4458abcdf2aa231f41e9c"
            ],
            "layout": "IPY_MODEL_e05debb4bb0b4cfbb6dc389cf6112846"
          }
        },
        "bd40f35353ef4f4ab7726e4c74807487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e3ff72510e498196d57cf3b329b628",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7b52a16db0f44606b4c224d45039c5fe",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "2b0d1c297e1640a7a3a80b1c6b76e534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf72c5e5c05d450db0935469a7cfcd97",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7063827023448eba3350947055d56eb",
            "value": 1
          }
        },
        "a90752c370f4458abcdf2aa231f41e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ad0c3936ac431a8932d5cff447c783",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_18ea3a866bdf44a6a3eb578206c91627",
            "value": "‚Äá4.20k/?‚Äá[00:00&lt;00:00,‚Äá234kB/s]"
          }
        },
        "e05debb4bb0b4cfbb6dc389cf6112846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87e3ff72510e498196d57cf3b329b628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b52a16db0f44606b4c224d45039c5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf72c5e5c05d450db0935469a7cfcd97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f7063827023448eba3350947055d56eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3ad0c3936ac431a8932d5cff447c783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18ea3a866bdf44a6a3eb578206c91627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72d947714a864d5eb9f5b358180ea8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1177bbbaba2a439a8985d1d8a660267d",
              "IPY_MODEL_c3e6937f5b904c458681490c59cac2e8",
              "IPY_MODEL_b819238879f845938d6118575d4f24c2"
            ],
            "layout": "IPY_MODEL_de391fcd29b94cb9a8c2b6277ee4d3c9"
          }
        },
        "1177bbbaba2a439a8985d1d8a660267d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca38afee5cb428491304027ac3f1bb3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6846ca6ae32e4cb88ed0ae01a6f21929",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "c3e6937f5b904c458681490c59cac2e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6c2377eb2b542f4ba48fba1467ab098",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edfa4eed9fd2406dbb9f344cc25fed29",
            "value": 1
          }
        },
        "b819238879f845938d6118575d4f24c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c4a77bf3c384aeca73f84cc400631cc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7df58d7bfea643a093e84959b8a8ff46",
            "value": "‚Äá6.79k/?‚Äá[00:00&lt;00:00,‚Äá373kB/s]"
          }
        },
        "de391fcd29b94cb9a8c2b6277ee4d3c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca38afee5cb428491304027ac3f1bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6846ca6ae32e4cb88ed0ae01a6f21929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6c2377eb2b542f4ba48fba1467ab098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "edfa4eed9fd2406dbb9f344cc25fed29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c4a77bf3c384aeca73f84cc400631cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df58d7bfea643a093e84959b8a8ff46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Face the Future Hackathon ‚Äî CIFAKE Image Classification  \n",
        "### Organized by IIIT Bangalore  \n",
        "#### Team **WBS**\n",
        "\n",
        "**Team Members:**\n",
        "- Siddartha A Y  \n",
        "- Kushal B Gowda  \n",
        "- Anish Kasetty  \n",
        "\n",
        "---\n",
        "\n",
        "## üèÜ Problem Overview\n",
        "\n",
        "In this challenge, we tackle the **CIFAKE Image Classification Problem**, where the objective is to build an AI model capable of **distinguishing between real and AI-generated (fake) images**.  \n",
        "The dataset consists of two classes:\n",
        "- üßç‚Äç‚ôÇÔ∏è **Real** ‚Äî Authentic, human-captured images.  \n",
        "- ü§ñ **Fake** ‚Äî Synthetic images generated using AI models such as Stable Diffusion or DALL¬∑E.  \n",
        "\n",
        "Our goal is to develop a robust model that can **reliably classify images as either real or fake**, even when facing high-quality synthetic data that closely mimics real-world imagery.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objective\n",
        "\n",
        "To train and evaluate multiple deep learning models ‚Äî such as **Vision Transformers (ViT)**, **ConvNeXt**, and others ‚Äî on the CIFAKE dataset, and to explore **ensemble strategies** that combine their strengths for better generalization and accuracy.\n",
        "\n",
        "Specifically, this notebook will:\n",
        "1. Preprocess the CIFAKE dataset and create efficient data pipelines.  \n",
        "2. Fine-tune multiple pretrained models on the binary classification task.  \n",
        "3. Evaluate individual model performance.  \n",
        "4. Implement an **ensemble** of models to improve test set predictions.  \n",
        "5. Generate and save the final predictions for submission.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Technical Stack\n",
        "\n",
        "- **Framework:** PyTorch  \n",
        "- **Libraries:** torchvision, timm, transformers, scikit-learn, numpy, pandas, matplotlib  \n",
        "- **Approach:** Transfer Learning + Ensembling  \n",
        "- **Hardware:** GPU-accelerated training on Google Colab  \n",
        "\n",
        "---\n",
        "\n",
        "## üìÑ Notebook Structure\n",
        "\n",
        "| Section | Description |\n",
        "|----------|-------------|\n",
        "| **1. Setup & Imports** | Installing dependencies and setting up environment. |\n",
        "| **2. Data Preparation** | Organizing dataset, applying transformations, and creating dataloaders. |\n",
        "| **3. Model Training** | Training individual deep learning models on CIFAKE. |\n",
        "| **4. Evaluation** | Analyzing validation performance metrics and visualizations. |\n",
        "| **5. Ensembling** | Combining multiple models for improved accuracy. |\n",
        "| **6. Inference & Submission** | Running predictions on test data and saving outputs. |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Motivation\n",
        "\n",
        "With the rise of AI-generated media, **detecting synthetic content** has become a crucial challenge for ensuring authenticity and trust in digital spaces.  \n",
        "Through this hackathon, Team WBS aims to contribute towards building robust systems that can **differentiate real from fake imagery** using advanced deep learning techniques.\n",
        "\n",
        "---\n",
        "\n",
        "> üöÄ *Let‚Äôs dive into building our CIFAKE classification pipeline step by step!*\n"
      ],
      "metadata": {
        "id": "gI_9p-AkH944"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_4rykV2umou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b88b641-6730-4db4-dd84-4e31c7dfde71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/iiitb/cleaned\" /content/cleaned && cp -r \"/content/drive/MyDrive/iiitb/test\" /content/cleaned/test"
      ],
      "metadata": {
        "id": "MVkPL74jyxqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Step 1: Install Required Libraries\n",
        "\n",
        "Before we begin model training, we need to install all the essential Python libraries that will be used throughout this notebook.  \n",
        "\n",
        "- **transformers** ‚Üí Provides access to state-of-the-art pre-trained models (like ViT, ConvNeXt) from Hugging Face.  \n",
        "- **datasets** ‚Üí Simplifies dataset handling, preprocessing, and loading.  \n",
        "- **timm** ‚Üí Contains a wide range of high-performance computer vision models optimized for PyTorch.  \n",
        "- **accelerate** ‚Üí Enables efficient multi-GPU or mixed precision training.  \n",
        "- **evaluate** ‚Üí Offers standardized metrics for model evaluation (e.g., accuracy, F1-score).  \n",
        "- **torchvision** ‚Üí Provides image transformations, datasets, and model utilities for PyTorch.  \n",
        "- **matplotlib** ‚Üí Used for visualizing training curves, sample images, and results.  \n",
        "- **scikit-learn** ‚Üí Offers additional metrics and tools for model evaluation and analysis.\n",
        "\n",
        "These installations ensure that our environment is fully equipped for fine-tuning and evaluating image classification models on the CIFAKE dataset."
      ],
      "metadata": {
        "id": "7ho3RoWQIPgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets timm accelerate evaluate torchvision\n",
        "!pip install -q matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXAUqW96G1C_",
        "outputId": "990f1dbe-943c-418b-bf50-a25b182ba5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Model 1 ‚Äî Vision Transformer (ViT-Base, Patch16-224-In21k)\n",
        "\n",
        "The **Vision Transformer (ViT)** is a state-of-the-art architecture introduced by Google that applies the **Transformer mechanism** ‚Äî originally designed for natural language processing ‚Äî to image data.  \n",
        "Instead of using traditional convolutional layers, ViT divides an image into small patches and processes them as a sequence of tokens, just like words in a sentence.\n",
        "\n",
        "### üîç Model Details\n",
        "- **Model Name:** `google/vit-base-patch16-224-in21k`  \n",
        "- **Patch Size:** 16√ó16 pixels  \n",
        "- **Input Resolution:** 224√ó224  \n",
        "- **Pretrained On:** ImageNet-21k (a large-scale dataset with ~14 million images and 21k classes)  \n",
        "\n",
        "### ‚öôÔ∏è Why ViT for CIFAKE?\n",
        "- ViT captures **global dependencies** in the image, making it highly effective for subtle differences between real and AI-generated (fake) images.  \n",
        "- The `in21k` variant benefits from **large-scale pretraining**, giving it strong generalization and feature extraction capabilities.  \n",
        "- Ideal for **transfer learning**, especially when dataset size is limited ‚Äî as in CIFAKE.\n",
        "\n",
        "In this section, we‚Äôll fine-tune the `ViT-Base-Patch16-224-In21k` model on our binary classification task to distinguish between **real** and **fake** images."
      ],
      "metadata": {
        "id": "SHukOKw1Ihp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßπ Data Preparation & Augmentation\n",
        "\n",
        "To train a robust image classification model, we first prepare our **training** and **validation** datasets using `torchvision.transforms` and `ImageFolder`.\n",
        "\n",
        "### üì∏ Image Transformations\n",
        "\n",
        "We apply a series of transformations to enhance the model‚Äôs ability to generalize and handle variations in real-world data.\n",
        "\n",
        "#### üîß Training Transformations\n",
        "\n",
        "\n",
        "* **RandomResizedCrop:** Randomly crops and resizes images for scale invariance.\n",
        "* **RandomHorizontalFlip:** Helps the model become orientation-agnostic.\n",
        "* **ColorJitter:** Slightly changes brightness, contrast, and saturation to simulate lighting variations.\n",
        "* **RandAugment:** Automatically applies random augmentations to increase data diversity.\n",
        "* **ToTensor & Normalize:** Converts images to tensors and normalizes pixel values to [-1, 1] for stable training.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### üß™ Validation Transformations\n",
        "Only resizing and normalization are applied here ‚Äî no random augmentations ‚Äî to ensure consistent evaluation.\n",
        "\n",
        "### üóÇÔ∏è Dataset Loading\n",
        "\n",
        "We use `datasets.ImageFolder()` to load images from the following structure:\n",
        "\n",
        "```\n",
        "cleaned/\n",
        " ‚îú‚îÄ‚îÄ train/\n",
        " ‚îÇ    ‚îú‚îÄ‚îÄ real/\n",
        " ‚îÇ    ‚îî‚îÄ‚îÄ fake/\n",
        " ‚îî‚îÄ‚îÄ val/\n",
        "      ‚îú‚îÄ‚îÄ real/\n",
        "      ‚îî‚îÄ‚îÄ fake/\n",
        "```\n",
        "\n",
        "### ‚öôÔ∏è Parameters\n",
        "\n",
        "* **IMG_SIZE = 224** ‚Üí Matches the input resolution expected by ViT.\n",
        "* **BATCH_SIZE = 16** ‚Üí Balances GPU memory and training speed.\n",
        "\n",
        "Finally, we print the number of samples in both training and validation sets to verify the data split.\n"
      ],
      "metadata": {
        "id": "VOyRz1c5Ir0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\"cleaned/train\", transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(\"cleaned/val\", transform=val_transform)\n",
        "\n",
        "len(train_dataset), len(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwg7GnLVG2G7",
        "outputId": "71590fed-2d50-45b6-c318-f0b0b30ee849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1599, 401)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßæ Converting ImageFolder Data to Hugging Face Dataset Format\n",
        "\n",
        "To efficiently integrate with the **Hugging Face training pipeline** (via the `Trainer` API), we convert our standard PyTorch `ImageFolder` data structure into the **`datasets.Dataset`** format.  \n",
        "This makes preprocessing, transformation, and batching simpler and more compatible with models from the `transformers` library.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Conversion Function: `from_imagefolder_to_hf()`\n",
        "\n",
        "**Purpose:**\n",
        "This function scans through the directory structure (e.g., `train/real`, `train/fake`) and collects all image file paths along with their class labels.\n",
        "It then constructs a **Hugging Face `Dataset`** object containing:\n",
        "\n",
        "* `\"image\"` ‚Üí file path to the image\n",
        "* `\"label\"` ‚Üí corresponding class index (0 for real, 1 for fake)\n",
        "\n",
        "This helps maintain consistent metadata and integrates seamlessly with downstream Hugging Face utilities.\n",
        "\n",
        "---\n",
        "\n",
        "### üß∞ Applying Image Transformations\n",
        "\n",
        "After loading the datasets, we apply the same **training** and **validation transformations** as defined earlier.\n",
        "\n",
        "* **`Image.open(x).convert(\"RGB\")`** ensures all images are in RGB format (3 channels).\n",
        "* The transformations convert raw image paths into ready-to-train tensors under the key `\"pixel_values\"`.\n",
        "* Using `.with_transform()` dynamically applies these transformations during training or evaluation.\n",
        "\n",
        "<br>\n",
        "\n",
        "The **collate function** ensures that each mini-batch returned by the DataLoader:\n",
        "\n",
        "* Combines individual image tensors into a single stacked tensor (`pixel_values`).\n",
        "* Converts the list of labels into a single tensor (`labels`).\n",
        "\n",
        "This structure is exactly what Hugging Face models expect as input.\n",
        "\n"
      ],
      "metadata": {
        "id": "rAIPfMwdJKdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset as HFDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def from_imagefolder_to_hf(path):\n",
        "    files, labels = [], []\n",
        "    classes = sorted(os.listdir(path))\n",
        "    class_to_idx = {c: i for i in classes}\n",
        "\n",
        "    for cls in classes:\n",
        "        for f in os.listdir(os.path.join(path, cls)):\n",
        "            files.append(os.path.join(path, cls, f))\n",
        "            labels.append(class_to_idx[cls])\n",
        "\n",
        "    return HFDataset.from_dict({\"image\": files, \"label\": labels})\n",
        "\n",
        "train_hf = from_imagefolder_to_hf(\"train\")\n",
        "val_hf = from_imagefolder_to_hf(\"val\")\n",
        "\n",
        "def apply_train_transform(batch):\n",
        "    batch[\"pixel_values\"] = [train_transform(Image.open(x).convert(\"RGB\")) for x in batch[\"image\"]]\n",
        "    return batch\n",
        "\n",
        "def apply_val_transform(batch):\n",
        "    batch[\"pixel_values\"] = [val_transform(Image.open(x).convert(\"RGB\")) for x in batch[\"image\"]]\n",
        "    return batch\n",
        "\n",
        "train_hf = train_hf.with_transform(apply_train_transform)\n",
        "val_hf = val_hf.with_transform(apply_val_transform)\n",
        "\n",
        "def collate_fn(examples):\n",
        "    return {\n",
        "        \"pixel_values\": torch.stack([e[\"pixel_values\"] for e in examples]),\n",
        "        \"labels\": torch.tensor([e[\"label\"] for e in examples])\n",
        "    }"
      ],
      "metadata": {
        "id": "U_7ES3s0HcHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Initializing the Vision Transformer (ViT) Model\n",
        "\n",
        "Now that our data pipeline is ready, we move on to **loading and initializing the pretrained Vision Transformer model** for our binary classification task.\n",
        "\n",
        "This simply hides unnecessary warning messages (e.g., about model configurations or deprecated parameters) to keep the notebook output clean and readable.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Loading the Pretrained ViT Model\n",
        "\n",
        "#### üîç Explanation:\n",
        "\n",
        "* **`AutoImageProcessor`** ‚Üí Handles all preprocessing required for the ViT model (e.g., resizing, normalization).\n",
        "* **`AutoModelForImageClassification`** ‚Üí Loads a pretrained ViT architecture ready for image classification tasks.\n",
        "\n",
        "#### ‚öôÔ∏è Key Parameters:\n",
        "\n",
        "* **`model_name`** ‚Üí Specifies the pretrained checkpoint (`google/vit-base-patch16-224-in21k`).\n",
        "* **`num_labels=2`** ‚Üí Since CIFAKE is a binary classification problem (Real vs Fake).\n",
        "* **`ignore_mismatched_sizes=True`** ‚Üí Allows loading models whose output layer shape differs from the pretrained one (since we redefine it for 2 classes).\n",
        "* **`use_fast=True`** ‚Üí Uses optimized (fast) preprocessing for quicker data handling.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Outcome\n",
        "\n",
        "At this stage, we have:\n",
        "\n",
        "* A **ViT backbone** pretrained on ImageNet-21k for powerful visual feature extraction.\n",
        "* A **custom classification head** fine-tuned for our binary task.\n",
        "* A corresponding **processor** to ensure all images are transformed exactly as the model expects.\n",
        "\n",
        "This setup forms the foundation for our training and fine-tuning process."
      ],
      "metadata": {
        "id": "_OyaUuH5JpCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(model_name,use_fast=True)\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eezDqv0BHpjU",
        "outputId": "b82116d2-943b-4056-a4cf-b82f97cf6989"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Freezing Early Layers of the Vision Transformer\n",
        "\n",
        "Before fine-tuning the ViT model on our CIFAKE dataset, we selectively **freeze the initial layers** of the transformer.  \n",
        "This helps retain the powerful, generic visual features learned during pretraining on ImageNet-21k while allowing deeper layers to adapt to our specific task.\n",
        "\n",
        "---\n",
        "#### üß† What This Does:\n",
        "\n",
        "* Iterates through all named parameters of the ViT model (`model.vit.named_parameters()`).\n",
        "* Checks if the layer name belongs to the **first three encoder blocks** or the **embedding layer**.\n",
        "* If so, sets `param.requires_grad = False`, meaning those parameters will **not** be updated during training.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Why We Do This\n",
        "\n",
        "1. **Retain Low-Level Features:**\n",
        "   The early ViT layers capture basic patterns like edges, colors, and textures ‚Äî which are useful for most visual tasks.\n",
        "2. **Faster Training:**\n",
        "   Fewer trainable parameters reduce computation time and memory usage.\n",
        "3. **Prevent Overfitting:**\n",
        "   Since CIFAKE has fewer images than ImageNet, freezing some layers prevents the model from overfitting to limited data.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Result\n",
        "\n",
        "After this step:\n",
        "\n",
        "* Only the **mid-to-high-level transformer layers** and the **classification head** remain trainable.\n",
        "* The model fine-tunes efficiently while leveraging the pretrained backbone‚Äôs strong feature representations."
      ],
      "metadata": {
        "id": "GLYJ8ELPKUs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.vit.named_parameters():\n",
        "    if any(k in name for k in [\"encoder.layer.0\", \"encoder.layer.1\", \"encoder.layer.2\", \"embeddings\"]):\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "0VFD8QaQHwM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Define Evaluation Metrics\n",
        "\n",
        "We define the metrics used to monitor model performance during training and validation.\n",
        "\n",
        "* **`accuracy`** ‚Üí Measures the overall percentage of correct predictions.\n",
        "* **`f1`** ‚Üí Balances precision and recall, useful for imbalanced datasets.\n",
        "* **`compute_metrics()`** ‚Üí Converts model logits to class predictions and returns both metrics.\n",
        "\n",
        "These metrics will be automatically computed by the Hugging Face `Trainer` after each evaluation step.\n"
      ],
      "metadata": {
        "id": "NfqOLc-LNVpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    acc = accuracy.compute(predictions=preds, references=p.label_ids)[\"accuracy\"]\n",
        "    f1_score = f1.compute(predictions=preds, references=p.label_ids)[\"f1\"]\n",
        "    return {\"accuracy\": acc, \"f1\": f1_score}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "dde929a129bb483191fdd0b39703f49f",
            "bd40f35353ef4f4ab7726e4c74807487",
            "2b0d1c297e1640a7a3a80b1c6b76e534",
            "a90752c370f4458abcdf2aa231f41e9c",
            "e05debb4bb0b4cfbb6dc389cf6112846",
            "87e3ff72510e498196d57cf3b329b628",
            "7b52a16db0f44606b4c224d45039c5fe",
            "bf72c5e5c05d450db0935469a7cfcd97",
            "f7063827023448eba3350947055d56eb",
            "a3ad0c3936ac431a8932d5cff447c783",
            "18ea3a866bdf44a6a3eb578206c91627",
            "72d947714a864d5eb9f5b358180ea8b6",
            "1177bbbaba2a439a8985d1d8a660267d",
            "c3e6937f5b904c458681490c59cac2e8",
            "b819238879f845938d6118575d4f24c2",
            "de391fcd29b94cb9a8c2b6277ee4d3c9",
            "0ca38afee5cb428491304027ac3f1bb3",
            "6846ca6ae32e4cb88ed0ae01a6f21929",
            "a6c2377eb2b542f4ba48fba1467ab098",
            "edfa4eed9fd2406dbb9f344cc25fed29",
            "7c4a77bf3c384aeca73f84cc400631cc",
            "7df58d7bfea643a093e84959b8a8ff46"
          ]
        },
        "id": "9WvGm0efIJRm",
        "outputId": "0c01fd2c-925c-4096-e4ec-68673bc7f6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dde929a129bb483191fdd0b39703f49f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72d947714a864d5eb9f5b358180ea8b6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Set Up Training Configuration\n",
        "\n",
        "We define all hyperparameters and training behaviors using Hugging Face‚Äôs `TrainingArguments`.\n",
        "\n",
        "### Key Parameters:\n",
        "\n",
        "* **`output_dir`** ‚Üí Saves checkpoints and logs.\n",
        "* **`*_strategy=\"epoch\"`** ‚Üí Evaluate, save, and log results at the end of each epoch.\n",
        "* **`learning_rate=3e-5`** ‚Üí Standard fine-tuning rate for pretrained transformers.\n",
        "* **`warmup_ratio=0.1`** ‚Üí Gradually increases learning rate at the start for stability.\n",
        "* **`weight_decay=0.05`** ‚Üí Prevents overfitting by penalizing large weights.\n",
        "* **`load_best_model_at_end=True`** ‚Üí Automatically restores the best-performing model.\n",
        "\n",
        "This configuration ensures a stable and well-monitored fine-tuning process for our ViT model.\n"
      ],
      "metadata": {
        "id": "ye0T8wGMNjtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit_base_results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=10,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.05,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "LLppPqNhIP_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Initialize Trainer and Start Fine-Tuning\n",
        "\n",
        "We now initialize the Hugging Face `Trainer`, which handles the entire training loop ‚Äî including optimization, evaluation, and checkpointing.\n",
        "\n",
        "### Components:\n",
        "\n",
        "* **`model`** ‚Üí The fine-tuned Vision Transformer.\n",
        "* **`training_args`** ‚Üí Contains all hyperparameters and strategies.\n",
        "* **`train_dataset` / `eval_dataset`** ‚Üí Training and validation data.\n",
        "* **`compute_metrics`** ‚Üí Evaluates accuracy and F1 after each epoch.\n",
        "* **`data_collator`** ‚Üí Prepares batches for training.\n",
        "\n",
        "This command begins the **fine-tuning process**, training the ViT model to distinguish between **real and fake** images.\n"
      ],
      "metadata": {
        "id": "VHw3bhRJN2Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "o-sWFSjvIS5o",
        "outputId": "9e0c8e6d-f628-4d8b-d4d0-ea4830204000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 10:14, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.683200</td>\n",
              "      <td>0.643752</td>\n",
              "      <td>0.745636</td>\n",
              "      <td>0.753623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.557600</td>\n",
              "      <td>0.383398</td>\n",
              "      <td>0.917706</td>\n",
              "      <td>0.921615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.362600</td>\n",
              "      <td>0.245775</td>\n",
              "      <td>0.935162</td>\n",
              "      <td>0.934010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.274000</td>\n",
              "      <td>0.205792</td>\n",
              "      <td>0.937656</td>\n",
              "      <td>0.936387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.220700</td>\n",
              "      <td>0.184484</td>\n",
              "      <td>0.940150</td>\n",
              "      <td>0.939698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.214900</td>\n",
              "      <td>0.184031</td>\n",
              "      <td>0.940150</td>\n",
              "      <td>0.941176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.187100</td>\n",
              "      <td>0.169492</td>\n",
              "      <td>0.955112</td>\n",
              "      <td>0.955665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.188500</td>\n",
              "      <td>0.165982</td>\n",
              "      <td>0.947631</td>\n",
              "      <td>0.948148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.157600</td>\n",
              "      <td>0.166511</td>\n",
              "      <td>0.945137</td>\n",
              "      <td>0.945545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.150900</td>\n",
              "      <td>0.166104</td>\n",
              "      <td>0.947631</td>\n",
              "      <td>0.947631</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.2997170095443726, metrics={'train_runtime': 615.933, 'train_samples_per_second': 25.961, 'train_steps_per_second': 1.624, 'total_flos': 1.239096913937326e+18, 'train_loss': 0.2997170095443726, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Model Performance Summary ‚Äî ViT-Base (Patch16-224-In21k)\n",
        "\n",
        "After training the Vision Transformer model for **10 epochs**, we observe steady improvements in both **validation accuracy** and **F1-score**, confirming effective fine-tuning on the CIFAKE dataset.\n",
        "\n",
        "| Epoch | Training Loss | Validation Loss | Accuracy | F1-Score |\n",
        "|:------:|:--------------:|:----------------:|:----------:|:----------:|\n",
        "| 1 | 0.6832 | 0.6438 | 0.7456 | 0.7536 |\n",
        "| 2 | 0.5576 | 0.3834 | 0.9177 | 0.9216 |\n",
        "| 3 | 0.3626 | 0.2458 | 0.9352 | 0.9340 |\n",
        "| 4 | 0.2740 | 0.2058 | 0.9377 | 0.9364 |\n",
        "| 5 | 0.2207 | 0.1845 | 0.9402 | 0.9397 |\n",
        "| 6 | 0.2149 | 0.1840 | 0.9402 | 0.9412 |\n",
        "| 7 | 0.1871 | 0.1695 | 0.9551 | 0.9557 |\n",
        "| 8 | 0.1885 | 0.1660 | 0.9476 | 0.9481 |\n",
        "| 9 | 0.1576 | 0.1665 | 0.9451 | 0.9455 |\n",
        "| 10 | 0.1509 | 0.1661 | 0.9476 | 0.9476 |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Observations:\n",
        "- The **training loss** steadily decreased from **0.68 ‚Üí 0.15**, showing good convergence.  \n",
        "- **Validation accuracy** peaked around **95.5%**, with an **F1-score of 0.956**, indicating excellent performance and balance between precision and recall.  \n",
        "- The **gap between training and validation loss** remained small, suggesting minimal overfitting.  \n",
        "- The model reached strong generalization capability after ~7 epochs.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary:\n",
        "The fine-tuned **ViT-Base (Patch16-224-In21k)** model achieved:\n",
        "- **Final Accuracy:** ‚âà **95.5%**  \n",
        "- **Final F1-Score:** ‚âà **0.956**  \n",
        "- **Total Training Time:** ~10 minutes (GPU runtime)  \n",
        "- **Outcome:** A highly reliable model for distinguishing **real vs. AI-generated images** in the CIFAKE dataset.\n"
      ],
      "metadata": {
        "id": "zMj1FiAEONT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Visualizing Training Progress\n",
        "\n",
        "To better understand the model‚Äôs learning behavior, we plot the **training vs validation loss** and the **validation accuracy** across epochs.\n",
        "\n",
        "### üîç Insights:\n",
        "\n",
        "* The **training and validation loss curves** both show a smooth downward trend, confirming stable learning.\n",
        "* **Validation accuracy** consistently improves and plateaus around 95%, showing convergence without overfitting.\n",
        "* The close alignment between train and validation curves indicates strong generalization of the ViT model.\n",
        "\n",
        "These plots visually confirm that the fine-tuning process was successful and well-regularized."
      ],
      "metadata": {
        "id": "RlStHSJVOXjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = trainer.state.log_history\n",
        "train_acc = [m[\"eval_accuracy\"] for m in metrics if \"eval_accuracy\" in m]\n",
        "train_loss = [m[\"loss\"] for m in metrics if \"loss\" in m]\n",
        "val_loss = [m[\"eval_loss\"] for m in metrics if \"eval_loss\" in m]\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss, label=\"Train Loss\")\n",
        "plt.plot(val_loss, label=\"Val Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss Curves\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_acc, label=\"Val Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "XEGVy_igIUbM",
        "outputId": "f2fc0cb2-38e4-409d-8848-0276b0808407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Saving the Best Model and Processor\n",
        "\n",
        "After training, we save both the **fine-tuned model** and its corresponding **image processor** for future inference or deployment.\n",
        "\n",
        "### üß† Why Save Both?\n",
        "\n",
        "* **Model (`trainer.save_model`)** ‚Üí Stores the trained weights and architecture.\n",
        "* **Processor (`processor.save_pretrained`)** ‚Üí Saves all preprocessing configurations (image size, normalization, etc.) needed for consistent inference.\n",
        "\n",
        "This ensures that the model can be **reloaded later** and used for prediction or ensembling without retraining.\n"
      ],
      "metadata": {
        "id": "4cRix-mcOlbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./vit_base_best\")\n",
        "processor.save_pretrained(\"./vit_base_best\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsJV_WsxLe_o",
        "outputId": "f1bea78f-2cb4-407d-e606-3301230c3325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vit_base_best/preprocessor_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Model 2 ‚Äî ConvNeXt (Base, 224, 22k)\n",
        "\n",
        "The **ConvNeXt** architecture, introduced by Meta AI, is a **next-generation convolutional neural network (CNN)** that modernizes classic CNN design principles with innovations inspired by Vision Transformers.  \n",
        "It reimagines standard convolutional blocks with large kernels, layer normalization, and residual scaling ‚Äî making it both **efficient** and **state-of-the-art**.\n",
        "\n",
        "### üîç Model Details\n",
        "- **Model Name:** `facebook/convnext-base-224-22k`  \n",
        "- **Input Resolution:** 224√ó224  \n",
        "- **Architecture Type:** Modern CNN (Transformer-inspired)  \n",
        "- **Pretrained On:** ImageNet-22k (a massive dataset with 22,000+ categories)  \n",
        "\n",
        "### ‚öôÔ∏è Why ConvNeXt for CIFAKE?\n",
        "- **Local Detail Awareness:** Convolutions capture intricate pixel-level details ‚Äî crucial for identifying fine artifacts that reveal fake images.  \n",
        "- **Large-Scale Pretraining:** Training on ImageNet-22k provides rich, diverse visual representations, improving transfer learning performance.  \n",
        "- **Efficient Fine-Tuning:** Achieves transformer-level accuracy with CNN-level efficiency and faster convergence.  \n",
        "\n",
        "In this section, we‚Äôll fine-tune the `ConvNeXt-Base-224-22k` model on the CIFAKE dataset as the **second member of our ensemble**.  \n",
        "While ViT provides a **global attention-based perspective**, ConvNeXt complements it with **powerful local texture analysis**, creating a more holistic ensemble for real vs. fake image classification."
      ],
      "metadata": {
        "id": "oQJ-_ldRO6c6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß© Importing Required Libraries\n",
        "\n",
        "In this section, we import all the essential libraries and modules required for **training and fine-tuning** our ConvNeXt model on the CIFAKE dataset. Each import serves a specific purpose in the deep learning workflow ‚Äî from data preprocessing to visualization and evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† Core Libraries\n",
        "\n",
        "* **`torch`**, **`numpy`** ‚Üí Fundamental tools for tensor manipulation and numerical computation.\n",
        "* **`os`**, **`PIL` (Python Imaging Library)** ‚Üí Handle image loading and file operations seamlessly.\n",
        "* **`tqdm`** ‚Üí Adds progress bars for monitoring training and data processing.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© Data Augmentation & Processing\n",
        "\n",
        "* **`albumentations`** ‚Üí Provides powerful and flexible image augmentation techniques to improve generalization.\n",
        "* **`ToTensorV2`** ‚Üí Converts augmented images into PyTorch tensors for model training.\n",
        "* **`datasets` (Hugging Face)** ‚Üí Simplifies dataset management and integration with the Transformers ecosystem.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Model & Training Utilities\n",
        "\n",
        "* **`transformers`** ‚Üí Core library for loading pretrained ConvNeXt models and managing the training pipeline.\n",
        "  Includes:\n",
        "\n",
        "  * `AutoImageProcessor` for preprocessing images\n",
        "  * `AutoModelForImageClassification` for loading pretrained ConvNeXt\n",
        "  * `TrainingArguments`, `Trainer` for handling training setup\n",
        "  * `EarlyStoppingCallback` and learning rate scheduler for optimization control\n",
        "\n",
        "---\n",
        "\n",
        "#### üìä Evaluation & Visualization\n",
        "\n",
        "* **`evaluate`** ‚Üí Computes standard metrics like accuracy and F1 with ease.\n",
        "* **`sklearn.metrics`** ‚Üí Provides additional evaluation metrics (precision, recall, F1).\n",
        "* **`matplotlib`** ‚Üí Used to visualize loss and accuracy curves for performance tracking.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ Together, these imports create a **robust and efficient environment** for model fine-tuning, monitoring, and evaluation ‚Äî ensuring smooth experimentation throughout the ConvNeXt training process.\n"
      ],
      "metadata": {
        "id": "KUXc3XQUPgPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForImageClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    get_cosine_schedule_with_warmup\n",
        ")\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "klbu0lKQcW6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üóÇÔ∏è Preparing the Dataset\n",
        "\n",
        "In this section, we **collect and label image paths** for both training and validation datasets from our preprocessed CIFAKE directory.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Step-by-Step Breakdown\n",
        "\n",
        "* **`glob`**: Scans directories and retrieves all `.png` file paths for each class (`real` and `fake`).\n",
        "* **Train / Validation Split**:\n",
        "\n",
        "  * `cleaned/train/real` and `cleaned/train/fake` ‚Üí Training images\n",
        "  * `cleaned/val/real` and `cleaned/val/fake` ‚Üí Validation images\n",
        "* **Label Assignment**:\n",
        "\n",
        "  * `real` ‚Üí Label **1**\n",
        "  * `fake` ‚Üí Label **0**\n",
        "* The lists `train_image_paths` and `val_image_paths` store image paths, while `train_labels` and `val_labels` store corresponding labels.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ This setup ensures a **clean and organized mapping** between images and their labels, forming the foundation for dataset loading and model training.\n"
      ],
      "metadata": {
        "id": "_mErl57qPzAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_paths = []  # e.g. list of paths to your training images\n",
        "train_labels = []       # e.g. list of 0/1 labels for each image\n",
        "val_image_paths = []    # e.g. list of paths to validation images\n",
        "val_labels = []\n",
        "\n",
        "# Example (if your folder is like dataset/train/real, dataset/train/fake):\n",
        "from glob import glob\n",
        "\n",
        "train_real = glob(\"cleaned/train/real/*.png\")\n",
        "train_fake = glob(\"cleaned/train/fake/*.png\")\n",
        "val_real = glob(\"cleaned/val/real/*.png\")\n",
        "val_fake = glob(\"cleaned/val/fake/*.png\")\n",
        "\n",
        "train_image_paths = train_real + train_fake\n",
        "train_labels = [1]*len(train_real) + [0]*len(train_fake)\n",
        "\n",
        "val_image_paths = val_real + val_fake\n",
        "val_labels = [1]*len(val_real) + [0]*len(val_fake)\n",
        "\n",
        "print(f\"Train: {len(train_image_paths)} | Val: {len(val_image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgdr614tcaVk",
        "outputId": "acba401f-153a-4723-cf8f-88410f05a3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1599 | Val: 401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üé® Data Augmentation & Preprocessing\n",
        "\n",
        "In this step, we define **image transformation pipelines** using the **Albumentations** library ‚Äî one for training and another for validation.\n",
        "These transformations enhance model generalization by simulating various visual conditions while maintaining consistency during evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† Training Transform (`train_transform`)\n",
        "\n",
        "This pipeline introduces **controlled randomness** to make the model robust and prevent overfitting:\n",
        "\n",
        "* **`RandomResizedCrop`** ‚Üí Randomly crops and resizes images to 224√ó224, encouraging the model to focus on different parts of the image.\n",
        "* **`HorizontalFlip (p=0.5)`** ‚Üí Mirrors images horizontally half the time, improving spatial invariance.\n",
        "* **`ColorJitter`** ‚Üí Slightly alters brightness, contrast, saturation, and hue to simulate lighting variations.\n",
        "* **`OneOf([...])`** ‚Üí Randomly applies **Gaussian Blur** or **Gauss Noise** (20% chance total), mimicking real-world image imperfections.\n",
        "* **`Normalize`** ‚Üí Standardizes pixel values to a common scale with mean and standard deviation `(0.5, 0.5, 0.5)`.\n",
        "* **`ToTensorV2`** ‚Üí Converts the final image to a PyTorch tensor ready for model input.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Validation Transform (`val_transform`)\n",
        "\n",
        "Unlike the training transform, this pipeline is **kept simple and deterministic** to ensure consistent evaluation:\n",
        "\n",
        "* **`Resize`** ‚Üí Scales images to 224√ó224 for uniform model input.\n",
        "* **`Normalize`** ‚Üí Applies the same normalization as in training.\n",
        "* **`ToTensorV2`** ‚Üí Converts images to tensors.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ Together, these transformations ensure that the **training data is diverse and realistic**, while **validation data remains stable and comparable**, enabling reliable model assessment.\n"
      ],
      "metadata": {
        "id": "zQNwwPb6QJqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose([\n",
        "    A.RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0)),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.3),\n",
        "    A.OneOf([\n",
        "        A.GaussianBlur(blur_limit=3, p=1),\n",
        "        A.GaussNoise(var_limit=(10, 20), p=1)\n",
        "    ], p=0.2),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "7-Y5LYkJcbIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß± Building a Custom PyTorch Dataset\n",
        "\n",
        "Here, we define a **custom dataset class** ‚Äî `ImageDataset` ‚Äî that efficiently loads and preprocesses images for both **training** and **validation** using PyTorch‚Äôs `Dataset` API.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Class Overview\n",
        "\n",
        "```python\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "```\n",
        "\n",
        "This subclass enables flexible loading of image data, compatible with PyTorch‚Äôs `DataLoader` for batching and shuffling.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© Key Components\n",
        "\n",
        "* **`__init__`**\n",
        "  Initializes the dataset with:\n",
        "\n",
        "  * `image_paths`: list of image file paths\n",
        "  * `labels`: corresponding numeric labels (0 = fake, 1 = real)\n",
        "  * `transform`: the Albumentations pipeline for data augmentation\n",
        "\n",
        "* **`__len__`**\n",
        "  Returns the total number of samples ‚Äî required for iteration and batching.\n",
        "\n",
        "* **`__getitem__`**\n",
        "  Loads an image from disk, converts it to RGB (to handle grayscale inconsistencies), applies the provided transformations, and returns a dictionary containing:\n",
        "\n",
        "  * `\"pixel_values\"` ‚Üí transformed image tensor\n",
        "  * `\"labels\"` ‚Üí corresponding class label as a PyTorch tensor\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† Dataset Creation\n",
        "\n",
        "```python\n",
        "train_dataset = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
        "val_dataset   = ImageDataset(val_image_paths, val_labels, transform=val_transform)\n",
        "```\n",
        "\n",
        "* `train_dataset` uses **augmentations** for robustness.\n",
        "* `val_dataset` uses **minimal transformations** for consistent evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ This modular dataset design ensures **scalability, flexibility, and seamless integration** with PyTorch‚Äôs training pipeline ‚Äî making it easy to plug into any model or training framework."
      ],
      "metadata": {
        "id": "YNU2yY49QRf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return {\"pixel_values\": image, \"labels\": torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "train_dataset = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
        "val_dataset = ImageDataset(val_image_paths, val_labels, transform=val_transform)"
      ],
      "metadata": {
        "id": "LZb9o3CFcnC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Loading the Pretrained ConvNeXt Model\n",
        "\n",
        "In this section, we load the **ConvNeXt-Base** model from Facebook‚Äôs repository ‚Äî a modern convolutional neural network designed to match Transformer-level performance while retaining CNN efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Step-by-Step Breakdown\n",
        "\n",
        "* **`model_name = \"facebook/convnext-base-224-22k\"`**\n",
        "  Specifies the pretrained model checkpoint.\n",
        "\n",
        "  * **Base variant** ‚Üí Balanced between accuracy and computational efficiency.\n",
        "  * **Pretrained on ImageNet-22K** ‚Üí Trained on a massive dataset (~14M images across 22,000 classes), giving it strong generalization capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "* **`AutoImageProcessor.from_pretrained(model_name)`**\n",
        "  Automatically loads the appropriate image preprocessing configuration (e.g., resizing, normalization) aligned with ConvNeXt‚Äôs training setup.\n",
        "\n",
        "---\n",
        "\n",
        "* **`AutoModelForImageClassification.from_pretrained(...)`**\n",
        "  Loads the ConvNeXt model architecture along with pretrained weights and adapts it for our task by:\n",
        "\n",
        "  * Setting **`num_labels=2`** for binary classification (`real` vs `fake`).\n",
        "  * Using **`ignore_mismatched_sizes=True`** to allow resizing of the classifier head to match our label count.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ This initialization step leverages **transfer learning** ‚Äî starting from a robust pretrained foundation and fine-tuning it for our specific CIFAKE classification challenge, improving both training speed and accuracy."
      ],
      "metadata": {
        "id": "fMhe6BYhQW1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/convnext-base-224-22k\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upSYJ4I8cq6M",
        "outputId": "0ea71729-9fed-43ef-be00-4c1c91af57a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-base-224-22k and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìè Defining the Evaluation Metric\n",
        "\n",
        "This section defines how the model‚Äôs performance will be **quantitatively evaluated** during training and validation.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Step-by-Step Explanation\n",
        "\n",
        "* **`evaluate.load(\"accuracy\")`**\n",
        "  Loads the **accuracy metric** from the ü§ó `evaluate` library ‚Äî a standardized and reliable metric for classification tasks.\n",
        "\n",
        "* **`compute_metrics(eval_pred)`**\n",
        "  Custom function used by the Hugging Face `Trainer` to compute metrics after each evaluation step:\n",
        "\n",
        "  1. **`logits, labels = eval_pred`** ‚Üí Extracts model predictions and true labels.\n",
        "  2. **`np.argmax(logits, axis=-1)`** ‚Üí Converts raw model outputs (logits) into predicted class indices.\n",
        "  3. **`metric_acc.compute(...)`** ‚Üí Calculates the **accuracy score** by comparing predicted labels against the true ones.\n",
        "\n",
        "\n",
        "> ‚úÖ This metric function ensures **consistent and automated accuracy tracking** throughout training ‚Äî a key measure of model reliability for the CIFAKE real vs fake classification task.\n"
      ],
      "metadata": {
        "id": "ugEAXadRRGEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "    return {\"accuracy\": acc}"
      ],
      "metadata": {
        "id": "J1CKqMdKdLlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Setting Up Training Configuration\n",
        "\n",
        "This block defines all **training hyperparameters and runtime settings** using the Hugging Face `TrainingArguments` class. These parameters control how the ConvNeXt model is fine-tuned, evaluated, and saved.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© Key Parameters Explained\n",
        "\n",
        "* **`output_dir=\"./convnext_results\"`** ‚Üí Directory where model checkpoints and logs will be saved.\n",
        "\n",
        "* **`eval_strategy=\"epoch\"` / `save_strategy=\"epoch\"`** ‚Üí Runs evaluation and saves model checkpoints **after each epoch**, ensuring regular validation tracking.\n",
        "\n",
        "* **`learning_rate=3e-5`** ‚Üí A low learning rate ideal for **fine-tuning** pretrained models without overfitting or catastrophic forgetting.\n",
        "\n",
        "* **`batch_size=16`** ‚Üí Controls the number of images processed simultaneously on each GPU step ‚Äî balances memory use and stability.\n",
        "\n",
        "* **`num_train_epochs=20`** ‚Üí Defines the total number of full passes through the training dataset.\n",
        "\n",
        "* **`weight_decay=0.05`** ‚Üí Adds mild regularization to prevent overfitting by penalizing large weights.\n",
        "\n",
        "* **`load_best_model_at_end=True`** ‚Üí Automatically restores the **best-performing model** (based on validation accuracy) after training.\n",
        "\n",
        "* **`metric_for_best_model=\"accuracy\"` & `greater_is_better=True`** ‚Üí Ensures that model selection prioritizes **higher accuracy** values.\n",
        "\n",
        "* **`fp16=True`** ‚Üí Enables **mixed precision training**, improving performance and reducing GPU memory usage.\n",
        "\n",
        "* **`lr_scheduler_type=\"cosine\"`** ‚Üí Uses a **cosine decay learning rate schedule** for smoother convergence.\n",
        "\n",
        "* **`label_smoothing_factor=0.1`** ‚Üí Helps the model generalize better by preventing overconfidence in predictions.\n",
        "\n",
        "* **`save_total_limit=2`** ‚Üí Retains only the **two best checkpoints**, saving storage space.\n",
        "\n",
        "* **`report_to=\"none\"`** ‚Üí Disables third-party logging integrations for a clean, local setup.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ **In essence**, these training arguments create a **balanced, efficient, and well-regularized fine-tuning setup** ‚Äî optimizing ConvNeXt‚Äôs learning dynamics for the CIFAKE binary classification challenge.\n"
      ],
      "metadata": {
        "id": "shJzjoDgTZtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "num_epochs = 20\n",
        "learning_rate = 3e-5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./convnext_results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.05,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    label_smoothing_factor=0.1,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "-nE81D5kdbNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üöÄ Initializing the Trainer  \n",
        "\n",
        "This block sets up the **Hugging Face `Trainer`**, which automates the complete training and evaluation process ‚Äî from data loading to logging and checkpointing ‚Äî making the workflow streamlined and reproducible.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Key Components  \n",
        "\n",
        "- **`model=model`** ‚Üí Specifies the pretrained **ConvNeXt-Base** model to be fine-tuned.  \n",
        "\n",
        "- **`args=training_args`** ‚Üí Passes in all hyperparameters and configurations defined earlier using `TrainingArguments`.  \n",
        "\n",
        "- **`train_dataset` & `eval_dataset`** ‚Üí Provide the prepared training and validation datasets with transformations applied.  \n",
        "\n",
        "- **`tokenizer=processor`** ‚Üí The image processor ensures that input preprocessing (resizing, normalization) aligns perfectly with ConvNeXt‚Äôs expected format.  \n",
        "\n",
        "- **`compute_metrics=compute_metrics`** ‚Üí Uses our custom metric function to compute **accuracy** after each evaluation cycle.  \n",
        "\n",
        "- **`callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]`** ‚Üí Implements **early stopping**, which halts training if the validation accuracy doesn‚Äôt improve for 3 consecutive epochs ‚Äî preventing overfitting and saving computation time.  \n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ The `Trainer` class handles **gradient computation, model updates, evaluation, and checkpointing** automatically ‚Äî offering a clean, high-level API for efficient fine-tuning of the ConvNeXt model on the CIFAKE dataset."
      ],
      "metadata": {
        "id": "iX6Ak4oXTgoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "I5NQKolDSPa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "3UY1zFOFdioX",
        "outputId": "dd30610e-1dfe-462d-ab1d-9c27a5f87adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/2000 11:11 < 07:28, 1.78 it/s, Epoch 12/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.446300</td>\n",
              "      <td>0.367195</td>\n",
              "      <td>0.915212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.368900</td>\n",
              "      <td>0.328680</td>\n",
              "      <td>0.927681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.294200</td>\n",
              "      <td>0.331570</td>\n",
              "      <td>0.922693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.255200</td>\n",
              "      <td>0.325151</td>\n",
              "      <td>0.932668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.256800</td>\n",
              "      <td>0.326747</td>\n",
              "      <td>0.922693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.321394</td>\n",
              "      <td>0.930175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.220500</td>\n",
              "      <td>0.317043</td>\n",
              "      <td>0.935162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.202700</td>\n",
              "      <td>0.316429</td>\n",
              "      <td>0.935162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.208400</td>\n",
              "      <td>0.307109</td>\n",
              "      <td>0.947631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>0.326934</td>\n",
              "      <td>0.927681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.201800</td>\n",
              "      <td>0.346192</td>\n",
              "      <td>0.925187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.203000</td>\n",
              "      <td>0.309794</td>\n",
              "      <td>0.940150</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Understanding the Evaluation Code\n",
        "\n",
        "This block processes the training logs and computes the **final training accuracy** to assess model performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Step-by-Step Breakdown\n",
        "\n",
        "1. **Extract Logs**\n",
        "   The code retrieves all recorded training and evaluation metrics from:\n",
        "\n",
        "   ```python\n",
        "   logs = trainer.state.log_history\n",
        "   ```\n",
        "\n",
        "   These logs contain information like training loss, validation loss, and validation accuracy for each epoch.\n",
        "\n",
        "2. **Parse Metrics**\n",
        "   From the logs, it separates key performance values:\n",
        "\n",
        "   * **`train_loss`** ‚Üí Training loss per epoch\n",
        "   * **`val_loss`** ‚Üí Validation loss per epoch\n",
        "   * **`val_acc`** ‚Üí Validation accuracy per epoch\n",
        "\n",
        "   This helps in visualizing learning trends later on.\n",
        "\n",
        "3. **Compute Final Train Accuracy**\n",
        "\n",
        "   * The model is switched to **evaluation mode** using `model.eval()`.\n",
        "   * The code iterates through the entire training dataset in batches.\n",
        "   * For each batch, it predicts class labels and compares them with the true labels.\n",
        "   * The overall accuracy is then calculated using:\n",
        "\n",
        "     ```python\n",
        "     accuracy_score(train_labels_acc, train_preds)\n",
        "     ```\n",
        "\n",
        "4. **Final Output**\n",
        "   The final line prints the **overall training accuracy** after fine-tuning:\n",
        "\n",
        "   ```python\n",
        "   ‚úÖ Final Train Accuracy: XX.XX%\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In short:**\n",
        "This block extracts metrics from the training process, organizes them for visualization, and computes how well the model learned the training data.\n"
      ],
      "metadata": {
        "id": "kSY1fARLUOoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs = trainer.state.log_history\n",
        "\n",
        "train_loss, val_loss, val_acc = [], [], []\n",
        "for log in logs:\n",
        "    if \"loss\" in log and \"epoch\" in log and \"eval_loss\" not in log:\n",
        "        train_loss.append((log[\"epoch\"], log[\"loss\"]))\n",
        "    if \"eval_loss\" in log:\n",
        "        val_loss.append((log[\"epoch\"], log[\"eval_loss\"]))\n",
        "        val_acc.append((log[\"epoch\"], log[\"eval_accuracy\"]))\n",
        "\n",
        "# Compute train accuracy manually (using last epoch model)\n",
        "train_preds, train_labels_acc = [], []\n",
        "model.eval()\n",
        "for batch in torch.utils.data.DataLoader(train_dataset, batch_size=32):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch[\"pixel_values\"].to(model.device))\n",
        "        preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
        "        train_preds.extend(preds)\n",
        "        train_labels_acc.extend(batch[\"labels\"].numpy())\n",
        "\n",
        "train_accuracy = accuracy_score(train_labels_acc, train_preds)\n",
        "print(f\"‚úÖ Final Train Accuracy: {train_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onClU2zUeByo",
        "outputId": "331d78fe-424e-4787-8829-e34e95fab791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final Train Accuracy: 99.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Model 2 ‚Äî ConvNeXt-Base (224, Pretrained on ImageNet-22K) ‚Äî Results Summary\n",
        "\n",
        "After fine-tuning the **ConvNeXt-Base** model on the CIFAKE dataset, the model demonstrated **strong and consistent performance** across training epochs, showing its effectiveness in distinguishing **real vs. AI-generated (fake)** images.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† Training Overview\n",
        "\n",
        "| Epoch | Training Loss | Validation Loss |  Accuracy  |\n",
        "| :---: | :-----------: | :-------------: | :--------: |\n",
        "|   1   |     0.4463    |      0.3672     |   0.9152   |\n",
        "|   4   |     0.2552    |      0.3251     |   0.9327   |\n",
        "|   8   |     0.2027    |      0.3164     |   0.9352   |\n",
        "|   9   |     0.2084    |      0.3071     | **0.9476** |\n",
        "|   12  |     0.2030    |      0.3098     |   0.9401   |\n",
        "\n",
        "---\n",
        "\n",
        "#### üìà Key Observations\n",
        "\n",
        "* **Steady Accuracy Improvement:**\n",
        "  The accuracy rose consistently from **91.5%** in the first epoch to a peak of **94.76%** by epoch 9 ‚Äî a clear sign of strong convergence.\n",
        "\n",
        "* **Stable Validation Loss:**\n",
        "  The validation loss gradually decreased and stabilized around **0.31**, indicating the model generalized well without significant overfitting.\n",
        "\n",
        "* **Early Stopping Efficiency:**\n",
        "  The **EarlyStoppingCallback** ensured optimal training duration by halting the process once performance plateaued, conserving compute resources.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© Performance Insight\n",
        "\n",
        "* ConvNeXt successfully **captured fine-grained texture and color cues**, making it highly effective for detecting subtle differences between real and synthetic images.\n",
        "* The **ImageNet-22K pretraining** provided a strong initialization, helping achieve near-saturation performance even on a relatively small binary dataset.\n",
        "* Results show ConvNeXt performs **on par with Vision Transformer (ViT)**, reinforcing its capability as a high-performance CNN alternative in modern vision tasks.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ **Final Verdict:**\n",
        "> The **ConvNeXt-Base** model achieved **~94.7% accuracy** on the validation set ‚Äî combining **Transformer-like representational power** with **CNN-level efficiency**. This makes it an excellent candidate for inclusion in our **ensemble** to further boost classification robustness."
      ],
      "metadata": {
        "id": "MEdyLr7ITmvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìà Visualizing Model Performance\n",
        "\n",
        "This section plots **training vs validation loss** and **accuracy curves** to analyze model learning behavior over epochs.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Step-by-Step Breakdown\n",
        "\n",
        "1. **Extract Epochs**\n",
        "\n",
        "   ```python\n",
        "   epochs = [x[0] for x in val_acc]\n",
        "   ```\n",
        "\n",
        "   Retrieves the list of epochs recorded during validation for consistent plotting.\n",
        "\n",
        "2. **Plot 1Ô∏è‚É£ ‚Äî Loss Curves**\n",
        "\n",
        "   * Plots **training loss** and **validation loss** across epochs.\n",
        "   * Helps visualize **how well the model is learning** and whether it‚Äôs **overfitting or underfitting**.\n",
        "\n",
        "   ```python\n",
        "   plt.plot(train_loss, label=\"Train Loss\")\n",
        "   plt.plot(val_loss, label=\"Val Loss\")\n",
        "   ```\n",
        "\n",
        "3. **Plot 2Ô∏è‚É£ ‚Äî Accuracy Curves**\n",
        "\n",
        "   * Displays **validation accuracy** over time.\n",
        "   * The red dashed line (`axhline`) marks the **final training accuracy**, allowing easy comparison between train and validation performance.\n",
        "\n",
        "4. **Styling & Display**\n",
        "\n",
        "   * Labels, titles, and legends make the plots easy to interpret.\n",
        "   * The `figsize=(12,5)` ensures clear side-by-side visualization.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In short:**\n",
        "These plots help evaluate the model‚Äôs **training stability, convergence, and generalization** performance.\n",
        "A small gap between train and validation curves indicates **good generalization**, while a widening gap may suggest **overfitting**.\n"
      ],
      "metadata": {
        "id": "709Gl1rWUcSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = [x[0] for x in val_acc]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot([x[0] for x in train_loss], [x[1] for x in train_loss], label=\"Train Loss\")\n",
        "plt.plot([x[0] for x in val_loss], [x[1] for x in val_loss], label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curves\"); plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, [x[1] for x in val_acc], label=\"Val Accuracy\")\n",
        "plt.axhline(y=train_accuracy, color=\"r\", linestyle=\"--\", label=\"Train Accuracy (Final)\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy Curves\"); plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Tuxn-Jdqhjtm",
        "outputId": "6d43b3df-58db-4305-e4ab-7aba2af88071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíæ Saving the Best ConvNeXt Model\n",
        "\n",
        "After training, we save both the **fine-tuned model** and its **image processor** for future inference or ensemble integration.\n",
        "\n",
        "```python\n",
        "trainer.save_model(\"./convnext_best_model\")\n",
        "processor.save_pretrained(\"./convnext_best_model\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† Explanation\n",
        "\n",
        "* **`trainer.save_model()`**\n",
        "  Saves the best-performing ConvNeXt model (based on validation accuracy) ‚Äî including its architecture and learned weights ‚Äî to the specified directory.\n",
        "\n",
        "* **`processor.save_pretrained()`**\n",
        "  Stores the corresponding image preprocessing pipeline (normalization, resizing, etc.) ensuring that future inputs are transformed **exactly the same way** as during training.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Result:**\n",
        "A complete, ready-to-use model stored in `./convnext_best_model`, containing both the **model** and **processor**, perfectly suited for later **evaluation, deployment, or ensembling**."
      ],
      "metadata": {
        "id": "IC1xp_BWUtou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./convnext_best_model\")\n",
        "processor.save_pretrained(\"./convnext_best_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLx0KpY9hldo",
        "outputId": "c31a54ee-49ed-4b9a-bfd4-b8e1a2736e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./convnext_best_model/preprocessor_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Model 3 ‚Äî EfficientNetV2-M (tf_efficientnetv2_m.in21k_ft_in1k)\n",
        "\n",
        "The **EfficientNetV2** family, developed by Google Research, represents a new generation of **highly efficient convolutional neural networks (CNNs)**.\n",
        "It is designed to achieve **state-of-the-art accuracy** while being **faster and more memory-efficient** than both the original EfficientNet and many transformer-based models.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Model Details\n",
        "\n",
        "* **Model Name:** `tf_efficientnetv2_m.in21k_ft_in1k`\n",
        "* **Architecture:** CNN-based with compound scaling of depth, width, and resolution\n",
        "* **Pretrained On:**\n",
        "\n",
        "  * **ImageNet-21K** for large-scale feature learning\n",
        "  * **Fine-tuned on ImageNet-1K** for strong downstream performance\n",
        "* **Input Resolution:** 224√ó224\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Why EfficientNetV2 for CIFAKE?\n",
        "\n",
        "* **Optimized for speed & accuracy** ‚Äî trains faster and generalizes better with fewer parameters.\n",
        "* **Excellent at capturing local visual textures**, making it ideal for detecting fine-grained cues that separate **real** from **AI-generated (fake)** images.\n",
        "* **Strong transfer learning performance** ‚Äî the dual-stage pretraining (IN21K ‚Üí IN1K) enhances adaptability on smaller, domain-specific datasets like CIFAKE.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Summary\n",
        "\n",
        "EfficientNetV2-M brings the **power of CNN efficiency** and **robust generalization** to our ensemble.\n",
        "It complements the **global attention** focus of ViT and the **hybrid convolutional power** of ConvNeXt ‚Äî making it a perfect third model for a **balanced and powerful ensemble**.\n"
      ],
      "metadata": {
        "id": "aC9_LF8nmjOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ Importing Required Libraries\n",
        "\n",
        "In this section, we import all the essential libraries needed to train and evaluate our **EfficientNetV2-M** model efficiently.\n",
        "\n",
        "### üß© Explanation\n",
        "\n",
        "* **`os`** ‚Üí Handles file and directory operations.\n",
        "* **`timm`** ‚Üí Provides access to the **EfficientNetV2** architecture and pretrained weights.\n",
        "* **`torch`, `torch.nn`, `DataLoader`** ‚Üí Core PyTorch modules for defining the model, layers, and loading data efficiently.\n",
        "* **`torchvision.datasets`** ‚Üí Simplifies loading datasets stored in folder structures.\n",
        "* **`timm.data.create_transform`** ‚Üí Creates standard data augmentation pipelines compatible with the model.\n",
        "* **`matplotlib.pyplot`** ‚Üí Used for visualizing training metrics and performance trends.\n",
        "* **`tqdm`** ‚Üí Adds clean and interactive progress bars to training loops for better monitoring.\n"
      ],
      "metadata": {
        "id": "u0jVQXtOVeIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from timm.data import create_transform\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "RMysc4RanyJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Defining Core Training Configuration\n",
        "\n",
        "This block sets up all the **key hyperparameters and paths** required for training the **EfficientNetV2-M** model. These parameters determine how the model learns, how data is processed, and where the best model checkpoint will be saved. Establishing these configurations upfront ensures **reproducibility** and **consistent training behavior** across runs.\n",
        "\n",
        "* **`BATCH_SIZE = 16`** ‚Üí Controls how many images are processed together in one iteration; balances GPU memory usage and training stability.\n",
        "* **`EPOCHS = 15`** ‚Üí Defines how many times the model will see the entire training dataset.\n",
        "* **`IMAGE_SIZE = 384`** ‚Üí Sets the input image resolution compatible with EfficientNetV2-M, allowing it to capture finer visual details.\n",
        "* **`LR = 3e-4`** ‚Üí The learning rate ‚Äî a crucial parameter that determines the step size in weight updates during optimization.\n",
        "* **`NUM_CLASSES = 2`** ‚Üí Indicates a binary classification task (real vs fake).\n",
        "* **`DEVICE`** ‚Üí Automatically selects GPU (`cuda`) if available, ensuring faster training; otherwise uses CPU.\n",
        "* **`TRAIN_DIR` / `VAL_DIR`** ‚Üí Paths pointing to the training and validation datasets, respectively.\n",
        "* **`SAVE_PATH`** ‚Üí Specifies where the best-performing model weights will be stored after training completion.\n"
      ],
      "metadata": {
        "id": "WChD7nxEV7rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 15\n",
        "IMAGE_SIZE = 384\n",
        "LR = 3e-4\n",
        "NUM_CLASSES = 2\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "TRAIN_DIR = \"cleaned/train\"\n",
        "VAL_DIR = \"cleaned/val\"\n",
        "SAVE_PATH = \"best_efficientnetv2_m.pth\""
      ],
      "metadata": {
        "id": "aWOilF97pVUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üé® Data Augmentation and Preprocessing\n",
        "\n",
        "This block defines **data transformations** applied to images before feeding them into the EfficientNetV2-M model. Proper augmentation helps improve the model‚Äôs **robustness** and **generalization**, while consistent preprocessing ensures inputs match the pretrained model‚Äôs expectations.\n",
        "\n",
        "* **`train_transform`** ‚Üí Defines the transformations applied to training images.\n",
        "\n",
        "  * **`input_size`** ‚Üí Sets the expected image shape `(3, 384, 384)` (RGB channels √ó height √ó width).\n",
        "  * **`is_training=True`** ‚Üí Enables data augmentation to introduce variability during training.\n",
        "  * **`auto_augment='rand-m9-mstd0.5-inc1'`** ‚Üí Applies a random augmentation policy that diversifies the data (e.g., color shifts, rotations).\n",
        "  * **`interpolation='bicubic'`** ‚Üí Ensures smooth resizing of images.\n",
        "  * **`re_prob=0.25`** & **`re_mode='pixel'`** ‚Üí Introduces *Random Erasing* with a 25% probability to prevent overfitting.\n",
        "\n",
        "* **`val_transform`** ‚Üí Defines transformations for validation images.\n",
        "\n",
        "  * **`is_training=False`** ‚Üí Disables augmentation to maintain consistent and unbiased evaluation.\n",
        "  * **`interpolation='bicubic'`** ‚Üí Keeps resizing consistent with training transformations.\n",
        "\n",
        "Together, these transformations ensure the model learns from varied yet standardized inputs, boosting overall accuracy and stability.\n"
      ],
      "metadata": {
        "id": "KDq3m0HOV__R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = create_transform(\n",
        "    input_size=(3, IMAGE_SIZE, IMAGE_SIZE),\n",
        "    is_training=True,\n",
        "    auto_augment='rand-m9-mstd0.5-inc1',\n",
        "    interpolation='bicubic',\n",
        "    re_prob=0.25,\n",
        "    re_mode='pixel',\n",
        "    re_count=1\n",
        ")\n",
        "\n",
        "val_transform = create_transform(\n",
        "    input_size=(3, IMAGE_SIZE, IMAGE_SIZE),\n",
        "    is_training=False,\n",
        "    interpolation='bicubic',\n",
        ")"
      ],
      "metadata": {
        "id": "4Ea1195wpXs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üóÇÔ∏è Dataset Preparation and Data Loading\n",
        "\n",
        "This block prepares the **training and validation datasets** and sets up efficient data loaders for batch processing during model training. It ensures that images are properly transformed, batched, and fed to the GPU with minimal latency.\n",
        "\n",
        "* **`datasets.ImageFolder()`** ‚Üí Automatically reads images from the specified folder structure (`TRAIN_DIR`, `VAL_DIR`) where subfolders (e.g., `real`, `fake`) define class labels.\n",
        "\n",
        "  * Applies the respective `train_transform` or `val_transform` to preprocess each image.\n",
        "\n",
        "* **`DataLoader()`** ‚Üí Efficiently loads data in batches for training and validation.\n",
        "\n",
        "  * **`batch_size=BATCH_SIZE`** ‚Üí Controls how many samples are processed per step.\n",
        "  * **`shuffle=True`** (for training) ‚Üí Randomizes image order each epoch for better generalization.\n",
        "  * **`num_workers=2`** ‚Üí Uses parallel CPU threads to speed up data loading.\n",
        "  * **`pin_memory=True`** ‚Üí Optimizes GPU data transfer for faster training.\n",
        "\n",
        "* **Print statements** ‚Üí Display dataset sizes and class names to verify successful loading and correct class mapping.\n",
        "\n",
        "‚úÖ *This setup ensures efficient, parallelized data feeding into the model ‚Äî a crucial step for smooth and scalable training.*"
      ],
      "metadata": {
        "id": "FSi9ODSFWHCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(VAL_DIR, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} images | Val: {len(val_dataset)} images\")\n",
        "print(f\"Classes: {train_dataset.classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60kLqhvwpbWi",
        "outputId": "44bcfd21-80ee-4fbd-b609-1613411d9424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1599 images | Val: 401 images\n",
            "Classes: ['fake', 'real']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Model Initialization ‚Äî EfficientNetV2-M\n",
        "\n",
        "This block loads and customizes the **EfficientNetV2-M** model architecture for our binary classification task (real vs fake).\n",
        "\n",
        "* **`timm.create_model('tf_efficientnetv2_m.in21k_ft_in1k', pretrained=True)`** ‚Üí Loads a pretrained **EfficientNetV2-M** model fine-tuned on ImageNet-1k, providing strong visual feature extraction from the start.\n",
        "* **`in_features = model.classifier.in_features`** ‚Üí Retrieves the number of input features to the model‚Äôs final classification layer.\n",
        "* **`model.classifier = nn.Linear(in_features, NUM_CLASSES)`** ‚Üí Replaces the original output layer with a new fully connected layer for **2 output classes** (real and fake).\n",
        "* **`model = model.to(DEVICE)`** ‚Üí Moves the model to GPU (`cuda`) if available for faster computation.\n",
        "\n",
        "‚úÖ *This setup effectively adapts a powerful pretrained EfficientNetV2-M backbone for our custom CIFAKE binary classification task.*"
      ],
      "metadata": {
        "id": "WzV9YWsWWJ9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = timm.create_model('tf_efficientnetv2_m.in21k_ft_in1k', pretrained=True)\n",
        "in_features = model.classifier.in_features\n",
        "model.classifier = nn.Linear(in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "yL-WR_XIpdXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Optimization Setup\n",
        "\n",
        "This block defines how the model learns ‚Äî specifying the **loss function**, **optimizer**, and **learning rate scheduler** for training the EfficientNetV2-M model.\n",
        "\n",
        "* **`criterion = nn.CrossEntropyLoss()`** ‚Üí Standard loss function for **multi-class (or binary)** classification, comparing predicted logits with true class labels.\n",
        "* **`optimizer = torch.optim.AdamW(...)`** ‚Üí Uses the **AdamW optimizer**, which combines adaptive learning rates with weight decay to prevent overfitting and improve convergence stability.\n",
        "\n",
        "  * **`lr=LR`** ‚Üí Sets the learning rate.\n",
        "  * **`weight_decay=1e-4`** ‚Üí Regularizes model weights to reduce overfitting.\n",
        "* **`scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(...)`** ‚Üí Gradually decreases the learning rate following a cosine curve, enabling smoother convergence and better fine-tuning during later epochs.\n",
        "\n",
        "‚úÖ *Together, these components ensure stable, efficient, and well-regularized training dynamics.*"
      ],
      "metadata": {
        "id": "gn2PxBVzWRjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
      ],
      "metadata": {
        "id": "KF3CGrG4pgKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üèãÔ∏è Model Training & Validation Loop\n",
        "\n",
        "This block handles the **entire training process** for the EfficientNetV2-M model ‚Äî including forward passes, backpropagation, validation, and best model saving. It iterates through multiple epochs, tracking performance at each step.\n",
        "\n",
        "#### üîÅ Key Steps\n",
        "\n",
        "* **Epoch Loop (`for epoch in range(EPOCHS)`):** Repeats training and validation for the specified number of epochs.\n",
        "\n",
        "* **Training Phase:**\n",
        "\n",
        "  * Sets the model to training mode (`model.train()`).\n",
        "  * For each batch, performs:\n",
        "\n",
        "    1. **Forward pass** ‚Üí computes predictions.\n",
        "    2. **Loss computation** ‚Üí measures prediction error using `CrossEntropyLoss`.\n",
        "    3. **Backward pass** ‚Üí computes gradients via `loss.backward()`.\n",
        "    4. **Optimizer step** ‚Üí updates model weights.\n",
        "  * Tracks cumulative **training loss** and **accuracy** across batches.\n",
        "\n",
        "* **Validation Phase:**\n",
        "\n",
        "  * Switches to evaluation mode (`model.eval()`) ‚Äî disables dropout and gradient computation.\n",
        "  * Evaluates the model on unseen validation data to measure **generalization**.\n",
        "  * Tracks **validation loss** and **accuracy** for each epoch.\n",
        "\n",
        "* **Learning Rate Scheduling:**\n",
        "\n",
        "  * `scheduler.step()` gradually adjusts the learning rate after each epoch for smooth optimization.\n",
        "\n",
        "* **Model Checkpointing:**\n",
        "\n",
        "  * Saves the model whenever validation accuracy improves (`if val_acc > best_val_acc:`).\n",
        "  * Ensures that only the **best-performing model** is retained (`SAVE_PATH`).\n",
        "\n",
        "‚úÖ *This structure ensures stable training, reliable validation tracking, and automatic saving of the optimal EfficientNetV2-M checkpoint.*"
      ],
      "metadata": {
        "id": "AauVNRWhWV6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_acc = 0\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nüöÄ Epoch {epoch+1}/{EPOCHS}\")\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= total\n",
        "    val_acc = correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
        "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), SAVE_PATH)\n",
        "        print(f\"‚úÖ Best model saved! (val_acc = {val_acc*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBjUt07TpkId",
        "outputId": "7f185c94-956c-4754-b320-def8896a20d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6494 | Train Acc: 64.10%\n",
            "Val Loss:   0.3929 | Val Acc:   83.79%\n",
            "‚úÖ Best model saved! (val_acc = 83.79%)\n",
            "\n",
            "üöÄ Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5566 | Train Acc: 70.73%\n",
            "Val Loss:   0.4319 | Val Acc:   78.80%\n",
            "\n",
            "üöÄ Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5436 | Train Acc: 72.61%\n",
            "Val Loss:   0.3572 | Val Acc:   85.54%\n",
            "‚úÖ Best model saved! (val_acc = 85.54%)\n",
            "\n",
            "üöÄ Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5051 | Train Acc: 75.11%\n",
            "Val Loss:   0.3442 | Val Acc:   83.79%\n",
            "\n",
            "üöÄ Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5000 | Train Acc: 74.48%\n",
            "Val Loss:   0.3098 | Val Acc:   88.78%\n",
            "‚úÖ Best model saved! (val_acc = 88.78%)\n",
            "\n",
            "üöÄ Epoch 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4849 | Train Acc: 76.11%\n",
            "Val Loss:   0.3595 | Val Acc:   86.03%\n",
            "\n",
            "üöÄ Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4611 | Train Acc: 79.11%\n",
            "Val Loss:   0.2969 | Val Acc:   87.78%\n",
            "\n",
            "üöÄ Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4237 | Train Acc: 80.43%\n",
            "Val Loss:   0.3346 | Val Acc:   86.53%\n",
            "\n",
            "üöÄ Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4388 | Train Acc: 79.92%\n",
            "Val Loss:   0.2451 | Val Acc:   92.27%\n",
            "‚úÖ Best model saved! (val_acc = 92.27%)\n",
            "\n",
            "üöÄ Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4059 | Train Acc: 81.55%\n",
            "Val Loss:   0.2772 | Val Acc:   90.77%\n",
            "\n",
            "üöÄ Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3824 | Train Acc: 82.43%\n",
            "Val Loss:   0.2705 | Val Acc:   89.53%\n",
            "\n",
            "üöÄ Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3710 | Train Acc: 83.18%\n",
            "Val Loss:   0.2447 | Val Acc:   91.02%\n",
            "\n",
            "üöÄ Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3483 | Train Acc: 85.37%\n",
            "Val Loss:   0.2490 | Val Acc:   90.77%\n",
            "\n",
            "üöÄ Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3514 | Train Acc: 83.68%\n",
            "Val Loss:   0.2511 | Val Acc:   90.77%\n",
            "\n",
            "üöÄ Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3414 | Train Acc: 85.12%\n",
            "Val Loss:   0.2475 | Val Acc:   91.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìà **Training Summary ‚Äî EfficientNetV2-M Fine-Tuning Results**  \n",
        "\n",
        "The **EfficientNetV2-M** model was fine-tuned over **15 epochs** for binary classification of real vs. fake images. The training process demonstrated **steady performance improvements**, with both training and validation metrics reflecting strong learning stability and generalization.  \n",
        "\n",
        "#### üîç **Performance Highlights**  \n",
        "- **Initial Phase (Epochs 1‚Äì5):**  \n",
        "  The model quickly adapted to the dataset, improving validation accuracy from **83.79% to 88.78%** while significantly reducing validation loss from **0.39 ‚Üí 0.31**. This indicates rapid learning and effective feature extraction from pre-trained weights.  \n",
        "\n",
        "- **Mid Phase (Epochs 6‚Äì10):**  \n",
        "  Accuracy gains continued with small oscillations due to the cosine learning rate schedule. The model achieved a breakthrough at **Epoch 9**, reaching a peak validation accuracy of **92.27%**, marking a clear improvement in generalization capability.  \n",
        "\n",
        "- **Final Phase (Epochs 11‚Äì15):**  \n",
        "  The model maintained **high stability**, with validation accuracy consistently hovering around **91‚Äì92%** and losses stabilizing near **0.24‚Äì0.25**. Training accuracy improved to **85.12%**, confirming balanced learning without signs of overfitting.  \n",
        "\n",
        "#### üèÖ **Best Model Checkpoint**\n",
        "- **Saved Model:** `best_efficientnetv2_m.pth`  \n",
        "- **Best Validation Accuracy:** **92.27% (Epoch 9)**  \n",
        "- **Best Validation Loss:** **0.2451**\n",
        "\n",
        "#### ‚öôÔ∏è **Overall Insight**\n",
        "EfficientNetV2-M effectively captured high-level semantic features while maintaining generalization on unseen data. The learning curve shows a **smooth and consistent convergence**, validating the strength of this architecture for real vs. fake image classification tasks.  \n",
        "\n",
        "‚úÖ *Result: A well-optimized, high-performing model with robust accuracy and minimal overfitting ‚Äî ideal for deployment or ensemble integration.*"
      ],
      "metadata": {
        "id": "zqWbH1RrWmoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "f8otz27NW6MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==="
      ],
      "metadata": {
        "id": "6qGUI3F3W8yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "v9UaoPPKW7jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ù **Model Ensembling ‚Äî Combining the Best of All Worlds**\n",
        "\n",
        "After independently fine-tuning three powerful vision models ‚Äî **ViT-Base**, **ConvNeXt-Base**, and **EfficientNetV2-M** ‚Äî we now move to the **ensemble phase**.\n",
        "\n",
        "The goal of ensembling is to **leverage the complementary strengths** of each architecture to produce more **robust, stable, and accurate predictions**.\n",
        "\n",
        "* **ViT (Vision Transformer)** captures long-range dependencies and global context.\n",
        "* **ConvNeXt** excels at hierarchical spatial feature extraction.\n",
        "* **EfficientNetV2-M** provides efficient scaling and fine-grained texture recognition.\n",
        "\n",
        "By **averaging or voting their outputs**, the ensemble can often outperform any single model ‚Äî reducing variance, improving generalization, and achieving state-of-the-art performance on the test set.\n",
        "\n",
        "üöÄ *In the following section, we‚Äôll load all three trained models, generate predictions, and combine them intelligently to form our final ensemble results.*\n"
      ],
      "metadata": {
        "id": "t1p-sR2Lwttx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ **Importing Dependencies**\n",
        "\n",
        "This section loads all necessary libraries for model ensembling and inference:\n",
        "\n",
        "* **PyTorch (`torch`, `torch.nn.functional`)** ‚Äî for tensor operations, model handling, and prediction computations.\n",
        "* **torchvision & DataLoader** ‚Äî to handle image datasets and batch loading efficiently.\n",
        "* **tqdm** ‚Äî for clean progress bars during batch processing.\n",
        "* **timm** ‚Äî to load models like *ConvNeXt* and *EfficientNetV2* easily.\n",
        "* **albumentations** ‚Äî for advanced image preprocessing and augmentation; includes `ToTensorV2` for PyTorch conversion.\n",
        "* **transformers** ‚Äî provides `AutoProcessor` and `AutoModelForImageClassification` for the *ViT* model.\n",
        "* **PIL (Image)** ‚Äî to open and process image files.\n",
        "* **NumPy** ‚Äî for numerical operations and array manipulations.\n",
        "* **json & os** ‚Äî for configuration management and file handling."
      ],
      "metadata": {
        "id": "Ua8g9U19XLQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from transformers import AutoProcessor, AutoModelForImageClassification\n",
        "from timm.data import create_transform\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "u-Sl4iWWwZ2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß© **Validation Transforms Setup**\n",
        "\n",
        "This block defines **three separate validation preprocessing pipelines**, one for each model type ‚Äî ensuring that each model receives input in its expected format for accurate inference.\n",
        "\n",
        "* **`VAL_SIZE` & `BATCH_SIZE`**:\n",
        "  Set the standardized image size (`224√ó224`) and batch size for inference across all models.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† 1. ViT (Vision Transformer)\n",
        "\n",
        "* Uses **`torchvision.transforms`**.\n",
        "* Resizes images to `224√ó224`, converts them to tensors, and normalizes pixel values to have mean and standard deviation of `0.5`.\n",
        "* This matches the ViT preprocessing standard used during training.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è 2. ConvNeXt\n",
        "\n",
        "* Uses **`albumentations`**, a more flexible augmentation library.\n",
        "* Performs resizing, normalization, and converts images to tensors using `ToTensorV2()`.\n",
        "* Ensures compatibility with ConvNeXt‚Äôs training pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "#### üöÄ 3. EfficientNetV2\n",
        "\n",
        "* Uses **`timm.create_transform()`**, which automatically aligns preprocessing with the model‚Äôs configuration.\n",
        "* Resizes images to `(3, 224, 224)` and applies bicubic interpolation for smoother scaling.\n",
        "* Maintains consistency with how EfficientNetV2 expects input during fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "‚ú® *Together, these transforms ensure all models receive clean, normalized, and appropriately scaled images ‚Äî a critical step before ensemble inference.*\n"
      ],
      "metadata": {
        "id": "vBWXgTuuXWMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Torchvision (for ViT)\n",
        "vit_val_transform = transforms.Compose([\n",
        "    transforms.Resize((VAL_SIZE, VAL_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Albumentations (for ConvNeXt)\n",
        "convnext_val_transform = A.Compose([\n",
        "    A.Resize(VAL_SIZE, VAL_SIZE),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Timm (for EfficientNet)\n",
        "eff_val_transform = create_transform(\n",
        "    input_size=(3, VAL_SIZE, VAL_SIZE),\n",
        "    is_training=False,\n",
        "    interpolation='bicubic'\n",
        ")"
      ],
      "metadata": {
        "id": "aqDsrYbOyu1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è **Model Loading for Ensemble**\n",
        "\n",
        "This section loads the **three trained models** (ViT, ConvNeXt, and EfficientNetV2) into memory and prepares them for inference ‚Äî ensuring consistency and compatibility for ensemble prediction.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† 1. Device Setup\n",
        "\n",
        "```python\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "```\n",
        "\n",
        "Automatically detects and uses the GPU if available, otherwise defaults to CPU.\n",
        "\n",
        "---\n",
        "\n",
        "#### üü£ 2. Vision Transformer (ViT)\n",
        "\n",
        "* Loads the **fine-tuned ViT model** and its **image processor** from the local directory `./vit_base_best`.\n",
        "* Moves the model to the chosen device and switches it to evaluation mode (`eval()`) to disable dropout and gradient tracking.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîµ 3. ConvNeXt\n",
        "\n",
        "* Loads the **fine-tuned ConvNeXt model** and its **processor** from `./convnext_best_model`.\n",
        "* Similarly, the model is transferred to the GPU (if available) and set to evaluation mode.\n",
        "\n",
        "---\n",
        "\n",
        "#### üü¢ 4. EfficientNetV2\n",
        "\n",
        "* Initializes an EfficientNetV2-M architecture with **2 output classes** (real vs. fake).\n",
        "* Loads the **best saved model weights** from `best_efficientnetv2_m.pth`.\n",
        "* Sends it to the correct device and sets it to evaluation mode for inference.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ *All three models are now properly loaded, synchronized, and ready for ensemble inference ‚Äî ensuring consistent and optimized performance across architectures.*"
      ],
      "metadata": {
        "id": "uXtnWagaXdsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load ViT\n",
        "vit_model = AutoModelForImageClassification.from_pretrained(\"./vit_base_best\").to(device)\n",
        "vit_processor = AutoProcessor.from_pretrained(\"./vit_base_best\")\n",
        "vit_model.eval()\n",
        "\n",
        "# Load ConvNeXt\n",
        "convnext_model = AutoModelForImageClassification.from_pretrained(\"./convnext_best_model\").to(device)\n",
        "convnext_processor = AutoProcessor.from_pretrained(\"./convnext_best_model\")\n",
        "convnext_model.eval()\n",
        "\n",
        "# Load EfficientNet\n",
        "eff_model = timm.create_model(\"tf_efficientnetv2_m.in21k_ft_in1k\", pretrained=False, num_classes=2)\n",
        "eff_model.load_state_dict(torch.load(\"best_efficientnetv2_m.pth\", map_location=device))\n",
        "eff_model = eff_model.to(device)\n",
        "eff_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SHJbk48GywLo",
        "outputId": "5a9a378a-87e9-4ec9-c742-b5ef1de08259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (conv_stem): Conv2dSame(3, 24, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "  (bn1): BatchNormAct2d(\n",
              "    24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "    (drop): Identity()\n",
              "    (act): SiLU(inplace=True)\n",
              "  )\n",
              "  (blocks): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): ConvBnAct(\n",
              "        (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): ConvBnAct(\n",
              "        (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): ConvBnAct(\n",
              "        (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): EdgeResidual(\n",
              "        (conv_exp): Conv2dSame(24, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): EdgeResidual(\n",
              "        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): EdgeResidual(\n",
              "        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (3): EdgeResidual(\n",
              "        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (4): EdgeResidual(\n",
              "        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): EdgeResidual(\n",
              "        (conv_exp): Conv2dSame(48, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(192, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): EdgeResidual(\n",
              "        (conv_exp): Conv2d(80, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): EdgeResidual(\n",
              "        (conv_exp): Conv2d(80, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (3): EdgeResidual(\n",
              "        (conv_exp): Conv2d(80, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (4): EdgeResidual(\n",
              "        (conv_exp): Conv2d(80, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): Identity()\n",
              "        (conv_pwl): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2dSame(320, 320, kernel_size=(3, 3), stride=(2, 2), groups=320, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(320, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(20, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(960, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (7): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (8): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (9): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (10): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (11): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (12): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (13): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2dSame(1056, 1056, kernel_size=(3, 3), stride=(2, 2), groups=1056, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1056, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (7): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (8): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (9): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (10): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (11): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (12): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (13): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (14): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (15): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (16): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (17): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1824, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv_head): Conv2d(512, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (bn2): BatchNormAct2d(\n",
              "    1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "    (drop): Identity()\n",
              "    (act): SiLU(inplace=True)\n",
              "  )\n",
              "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
              "  (classifier): Linear(in_features=1280, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ **Test-Time Augmentation (TTA) Function**\n",
        "\n",
        "This function enhances prediction robustness by averaging model outputs over multiple augmented versions of the same image.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© **Function:** `tta_predictions(model, images, num_augmentations=4)`\n",
        "\n",
        "Performs **Test-Time Augmentation (TTA)** ‚Äî a technique where images are slightly transformed multiple times during inference to stabilize predictions.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è **Step-by-Step Explanation**\n",
        "\n",
        "* **`model.eval()`**\n",
        "  Ensures the model runs in evaluation mode ‚Äî turning off dropout and batch normalization updates for consistent outputs.\n",
        "\n",
        "* **`torch.no_grad()`**\n",
        "  Disables gradient computation to save memory and speed up inference.\n",
        "\n",
        "* **`probs_sum = 0`**\n",
        "  Initializes a sum accumulator to average probabilities across augmentations.\n",
        "\n",
        "* **Loop over `num_augmentations` (default = 4)**\n",
        "  Each iteration:\n",
        "\n",
        "  1. Passes images through the model.\n",
        "  2. Extracts logits (handling both PyTorch and Hugging Face output types).\n",
        "  3. Applies a **softmax** to convert logits into probabilities.\n",
        "  4. Adds them to `probs_sum`.\n",
        "\n",
        "* **`return probs_sum / num_augmentations`**\n",
        "  Averages the probability distributions from all augmentations to get a more stable final prediction.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In summary:**\n",
        "This function makes model predictions **more reliable and noise-resistant** by leveraging minor input variations during inference ‚Äî a simple yet powerful ensemble trick at the sample level.\n"
      ],
      "metadata": {
        "id": "8WZhFXwpXj8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tta_predictions(model, images, num_augmentations=4):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        probs_sum = 0\n",
        "        for _ in range(num_augmentations):\n",
        "            logits = model(images)\n",
        "            # handle Hugging Face ImageClassifierOutput\n",
        "            if hasattr(logits, \"logits\"):\n",
        "                logits = logits.logits\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            probs_sum += probs\n",
        "        return probs_sum / num_augmentations"
      ],
      "metadata": {
        "id": "bTdNaM4LyyZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ **Ensemble Prediction Function**\n",
        "\n",
        "This function combines predictions from multiple models to produce a **more stable and accurate final output** through averaging ‚Äî a technique known as **model ensembling**.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© **Function:** `ensemble_predict(model_list, dataloader, device, label_map=None, test_mode=False)`\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è **Step-by-Step Breakdown**\n",
        "\n",
        "* **Inputs:**\n",
        "\n",
        "  * `model_list`: A list of `(model, name)` tuples containing all trained models.\n",
        "  * `dataloader`: The dataset loader (validation or test set).\n",
        "  * `device`: The computing device (CPU/GPU).\n",
        "  * `label_map`: (Optional) Converts numeric predictions into class names.\n",
        "  * `test_mode`: Whether to return filenames (for test set predictions).\n",
        "\n",
        "---\n",
        "\n",
        "#### üîÑ **Process:**\n",
        "\n",
        "1. **Loop through batches of images:**\n",
        "\n",
        "   * If `test_mode=True`, extract both images and filenames.\n",
        "   * Move images to the correct device (`cuda` or `cpu`).\n",
        "\n",
        "2. **Initialize `ensemble_probs`:**\n",
        "\n",
        "   * Used to accumulate probability predictions from all models.\n",
        "\n",
        "3. **Iterate through each model:**\n",
        "\n",
        "   * Call `tta_predictions(model, images)` to get averaged softmax probabilities.\n",
        "   * Add these probabilities across all models.\n",
        "\n",
        "4. **Average the results:**\n",
        "\n",
        "   * Divide by the number of models to get the **mean ensemble probability**.\n",
        "\n",
        "5. **Collect all probabilities:**\n",
        "\n",
        "   * Concatenate batch outputs into a single tensor.\n",
        "\n",
        "6. **Convert to class predictions:**\n",
        "\n",
        "   * Use `argmax` to get the final predicted class for each image.\n",
        "   * Optionally map numeric labels to their string class names.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† **Output:**\n",
        "\n",
        "* If **`test_mode=False`** ‚Üí Returns predicted class indices.\n",
        "* If **`test_mode=True`** ‚Üí Returns `(predictions, filenames)` for submission or reporting.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In summary:**\n",
        "This function fuses multiple model outputs (ViT, ConvNeXt, EfficientNet) into a **single, consensus prediction**, improving overall reliability and performance through **model diversity averaging**."
      ],
      "metadata": {
        "id": "lAfikAFCXvZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predict(model_list, dataloader, device, label_map=None, test_mode=False):\n",
        "    all_probs = []\n",
        "    filenames = []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Running Ensemble\"):\n",
        "        if test_mode:\n",
        "            images, names = batch\n",
        "            filenames.extend(names)\n",
        "        else:\n",
        "            images, _ = batch\n",
        "\n",
        "        images = images.to(device)\n",
        "        ensemble_probs = None\n",
        "\n",
        "        for model, name in model_list:\n",
        "            probs = tta_predictions(model, images)\n",
        "            if ensemble_probs is None:\n",
        "                ensemble_probs = probs\n",
        "            else:\n",
        "                ensemble_probs += probs\n",
        "\n",
        "        ensemble_probs /= len(model_list)\n",
        "        all_probs.append(ensemble_probs.cpu())\n",
        "\n",
        "    all_probs = torch.cat(all_probs)\n",
        "    preds = all_probs.argmax(dim=1).numpy()\n",
        "\n",
        "    if label_map:\n",
        "        preds = [label_map[p] for p in preds]\n",
        "\n",
        "    if test_mode:\n",
        "        return preds, filenames\n",
        "    else:\n",
        "        return preds"
      ],
      "metadata": {
        "id": "1GIh7md4y4U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßæ **Dataset Preparation for Ensemble Evaluation**\n",
        "\n",
        "This section prepares both the **validation** and **unlabeled test datasets** ‚Äî essential for evaluating and testing the ensemble model consistently across all architectures.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìÇ **1. Validation Dataset (Labeled)**\n",
        "\n",
        "* Uses **`torchvision.datasets.ImageFolder`**, which automatically maps folder names (`real`, `fake`) to numeric class indices.\n",
        "* The **validation loader** (`val_loader`) enables batch-wise inference.\n",
        "* `label_map` reverses the mapping (e.g., `{0: 'fake', 1: 'real'}`) for easy interpretation of predictions.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© **2. Custom Dataset for Unlabeled Test Images**\n",
        "\n",
        "* A custom **`UnlabeledImageDataset`** class handles test images that have **no labels**.\n",
        "* It:\n",
        "\n",
        "  * Loads all image file paths from the specified test directory.\n",
        "  * Opens and converts each image to RGB using **PIL**.\n",
        "  * Applies the given transformation (`vit_val_transform`).\n",
        "  * Returns the processed image **and its filename** ‚Äî useful for saving predictions later.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è **3. Test DataLoader**\n",
        "\n",
        "* The test dataset is wrapped in a **DataLoader** (`test_loader`) for efficient batch inference.\n",
        "* No shuffling is used since order consistency matters for saving results.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In short:**\n",
        "This block sets up a clean pipeline to feed both **labeled (validation)** and **unlabeled (test)** data into the ensemble for evaluation, ensuring consistency in preprocessing and easy mapping of predictions back to filenames."
      ],
      "metadata": {
        "id": "FDYwXMvLX5HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# ‚úÖ Validation dataset (still class-based)\n",
        "val_dataset = datasets.ImageFolder(\"cleaned/val\", transform=vit_val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "label_map = {v: k for k, v in val_dataset.class_to_idx.items()}\n",
        "\n",
        "# ‚úÖ Custom dataset for unlabeled test folder\n",
        "class UnlabeledImageDataset(Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.folder = folder\n",
        "        self.transform = transform\n",
        "        self.image_paths = sorted([\n",
        "            os.path.join(folder, fname)\n",
        "            for fname in os.listdir(folder)\n",
        "            if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, os.path.basename(img_path)  # returning filename for saving results later\n",
        "\n",
        "# ‚úÖ Load test dataset\n",
        "test_dataset = UnlabeledImageDataset(\"test\", transform=vit_val_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "uyFV0PBmy5qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ù **Running the Ensemble on Validation Data**\n",
        "\n",
        "This block executes the **ensemble prediction** on the labeled validation set:\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© **Steps:**\n",
        "\n",
        "1. **`model_list`**\n",
        "\n",
        "   * Combines the three trained models (ViT, ConvNeXt, EfficientNetV2) into a single list for ensemble inference.\n",
        "   * Each tuple contains `(model, name)` for identification.\n",
        "\n",
        "2. **`ensemble_predict(...)`**\n",
        "\n",
        "   * Feeds the validation data through all models.\n",
        "   * Uses **Test-Time Augmentation (TTA)** internally to stabilize predictions.\n",
        "   * Averages outputs from all models to produce **consensus predictions**.\n",
        "\n",
        "3. **`val_preds`**\n",
        "\n",
        "   * Stores the final predicted classes for the validation dataset.\n",
        "   * Uses `label_map` to convert numeric labels back to class names (`real` or `fake`).\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Outcome:**\n",
        "`val_preds` now contains the **ensemble‚Äôs predicted labels** for all validation images, combining the strengths of all three models for improved accuracy and reliability.\n"
      ],
      "metadata": {
        "id": "48AYfj5qYBD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = [\n",
        "    (vit_model, \"vit\"),\n",
        "    (convnext_model, \"convnext\"),\n",
        "    (eff_model, \"efficientnet\")\n",
        "]\n",
        "\n",
        "val_preds = ensemble_predict(model_list, val_loader, device, label_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZrpmoRYzTju",
        "outputId": "beb9d9f9-36f9-49a7-cf46-56fdaf8acf6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Ensemble: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:44<00:00,  1.70s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä **Evaluating Ensemble Performance on Validation Set**\n",
        "\n",
        "This block computes the **accuracy of the ensemble predictions** against the true labels from the validation dataset.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© **Steps:**\n",
        "\n",
        "1. **Extract True Labels:**\n",
        "\n",
        "   * `val_dataset.samples` provides a list of `(file_path, label_index)` for all validation images.\n",
        "   * Convert numeric indices to **class names** using `val_dataset.classes` for easy comparison.\n",
        "\n",
        "2. **Compute Accuracy:**\n",
        "\n",
        "   * Uses `sklearn.metrics.accuracy_score` to calculate the proportion of correctly predicted labels.\n",
        "   * Compares the **ensemble predictions (`val_preds`)** against the **true labels**.\n",
        "\n",
        "3. **Result:**\n",
        "\n",
        "   * Ensemble achieves **94.26% validation accuracy**, demonstrating a clear improvement over individual model performance.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Insight:**\n",
        "Combining ViT, ConvNeXt, and EfficientNetV2 in an ensemble **enhances robustness and accuracy**, successfully leveraging the complementary strengths of each architecture."
      ],
      "metadata": {
        "id": "F-pn7-wAYH6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Get true labels from val dataset\n",
        "true_labels = [label for _, label in val_dataset.samples]  # file path, label_idx\n",
        "true_labels = [val_dataset.classes[idx] for idx in true_labels]  # convert to class names\n",
        "\n",
        "# Compute accuracy\n",
        "acc = accuracy_score(true_labels, val_preds)\n",
        "print(f\"‚úÖ Ensemble Validation Accuracy: {acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZZEGSU4zY_z",
        "outputId": "cc50da84-7cc0-45dd-8916-303a576f3ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ensemble Validation Accuracy: 94.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíæ **Saving Final Ensemble Predictions**\n",
        "\n",
        "This block **creates and saves the final test set predictions** generated by the ensemble model:\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© **Steps:**\n",
        "\n",
        "1. **Generate Ensemble Predictions:**\n",
        "\n",
        "   * `ensemble_predict(...)` with `test_mode=True` produces:\n",
        "\n",
        "     * `test_preds`: Predicted labels for all test images.\n",
        "     * `test_files`: Corresponding filenames (for reference).\n",
        "\n",
        "2. **Format Predictions for Export:**\n",
        "\n",
        "   * Each prediction is paired with an **index** for clarity.\n",
        "   * Creates a list of dictionaries:\n",
        "\n",
        "     ```json\n",
        "     [{\"index\": 1, \"label\": \"real\"}, {\"index\": 2, \"label\": \"fake\"}, ...]\n",
        "     ```\n",
        "\n",
        "3. **Save to JSON:**\n",
        "\n",
        "   * Writes the formatted predictions to `results.json` with proper indentation for readability.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Outcome:**\n",
        "The **final test predictions are successfully saved** in `results.json`, ready for submission or downstream use.\n",
        "`results.json` now contains the **ensemble‚Äôs consensus labels** for all test images.\n"
      ],
      "metadata": {
        "id": "zxmv6Go3Yx_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds, test_files = ensemble_predict(model_list, test_loader, device, label_map, test_mode=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U62TnZ_90Ovx",
        "outputId": "36299783-aa3e-48fc-e7c2-191c7059a31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Ensemble: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:57<00:00,  1.79s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = [\n",
        "    {\"index\": i+1, \"label\": pred}\n",
        "    for i, pred in enumerate(test_preds)\n",
        "]\n",
        "\n",
        "with open(\"results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Saved final ensemble predictions to results.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAwpCFAq0SaK",
        "outputId": "ad5603ff-f3e7-4df9-fe81-1007a9626023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved final ensemble predictions to results.json\n"
          ]
        }
      ]
    }
  ]
}